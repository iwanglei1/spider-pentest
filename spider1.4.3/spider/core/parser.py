# date: 2020/1/08 10:09
# author: liyong
from .utils import check_soup, print_progress
from .const import failed_soup


class Parser(object):
    """解析器，通过beautiful soup解析HTML页面"""
    _site_title = 'SiteTitle'
    _nvd_site = 'https://nvd.nist.gov/vuln/detail/'


class ParserCNNVD(Parser):
    """CNNVD解析器"""
    _site_title = 'CNNVD'
    _site_head = 'http://www.cnnvd.org.cn'

    def parse_simple(self, simple_html):
        """解析所有页的简要数据"""
        parse_data = []
        total = len(simple_html)
        if total == 0:
            return parse_data
        print(f'* 开始解析采集的{total}页简要数据.')
        for index, page_soup in enumerate(simple_html):
            print_progress('正在解析简要数据', index, total)
            page_list = self.__parse_one_page_simple(page_soup)
            parse_data.extend(page_list)
        print(f'\r* {total}页{len(parse_data)}条简要数据解析完成.')
        return parse_data

    def parse_detail(self, detail_html):
        """解析所有页的详情数据"""
        parse_data = []
        total = len(detail_html)
        if total == 0:
            return parse_data
        print(f'* 开始解析{total}条的详情数据.')
        for index, page_soup in enumerate(detail_html):
            print_progress('正在解析详情数据', index, total)
            page_list = self.__parse_one_line_detail(page_soup)
            parse_data.append(page_list)
        print(f'\r* {total}条详情数据解析完成.')
        return parse_data

    def parse_cve(self, cve_html):
        """解析所有页的CVE数据"""
        parse_data = []
        total = len(cve_html)
        if total == 0:
            return parse_data
        print(f'* 开始解析{total}条的CVE评分数据.')
        for index, page_soup in enumerate(cve_html):
            print_progress('正在解析详情数据', index, total)
            page_list = self.__parse_one_cve_score(page_soup)
            parse_data.append(page_list)
        print(f'\r* {total}条CVE评分数据解析完成.')
        return parse_data

    def __parse_one_page_simple(self, page_soup):
        """解析一页简要数据"""
        one_page = []
        div_tag = page_soup.find('div', class_='list_list')
        if not div_tag:
            return one_page
        li_list = div_tag.select('div ul li')
        for li in li_list:
            tag_name = li.find('a', class_='a_title2')
            cnnvd_href = ''
            cnnvd_name = ''
            if tag_name:
                cnnvd_href = self._site_head + tag_name.get('href')
                cnnvd_name = tag_name.get_text().strip()
            li_p = li.find('p')
            li_p_a = li_p.find('a')
            cnnvd_id = li_p_a.get_text().strip()
            release_date = li.find('div', class_='fr').get_text().strip()
            cnnvd_level = li.find('img').get('title')
            one_page.append([cnnvd_id, cnnvd_name, cnnvd_level, release_date, cnnvd_href])
        return one_page

    def __parse_one_line_detail(self, html_data):
        """解析一条数据的详情数据"""
        if check_soup(html_data):
            return [failed_soup, failed_soup, failed_soup, failed_soup, failed_soup]
        info_detail = html_data.find('div', class_='detail_xq')
        if not info_detail:
            return ['NULL', 'NULL', 'NULL', 'NULL', 'NULL']
        cnnvd_title = info_detail.find('h2').get_text().strip()
        info_list = info_detail.select('ul li')
        cve_id = info_list[2].find('a').get_text().strip()
        cve_href = f'{self._nvd_site}{cve_id}'
        update_date = info_list[6].find('a').get_text().strip()

        vul_info = html_data.find_all('div', class_='d_ldjj')

        vul_des = ''  # 漏洞简介
        vul_des_array = vul_info[0].select('p')
        for index, p in enumerate(vul_des_array):
            vul_des += p.get_text().strip() + ('\n' if (index + 1) < len(vul_des_array) else '')

        vul_notice = ''  # 漏洞公告
        vul_notice_array = vul_info[1].select('p')
        for index, p in enumerate(vul_notice_array):
            vul_notice += p.get_text().strip() + ('\n' if (index + 1) < len(vul_notice_array) else '')
        return [cnnvd_title, cve_id, update_date, cve_href, vul_des, vul_notice]

    def __parse_one_cve_score(self, html_data):
        if check_soup(html_data):
            return [failed_soup, failed_soup, failed_soup]
        cve_detail = html_data.find('span', class_='severityDetail')
        if not cve_detail:
            # 当cve网站查不到该cve编号时，将不存在的分数及等级置为NULL
            return ['NULL', 'NULL', 'NULL']
        # 当cve网站可以查到该cve编号时，将未评定的分数及等级置为N/A
        cve_score = cve_detail.get_text().strip()
        cve_level = 'N/A'
        _score_detail = cve_score.split(' ')
        if len(_score_detail) == 2:
            cve_score = _score_detail[0]
            cve_level = self.__format_cve_level(_score_detail[1])
        cve_cvss_html = html_data.find('span', class_='tooltipCvss3NistMetrics')
        cve_cvss = 'N/A'
        if cve_cvss_html:
            cve_cvss = cve_cvss_html.get_text().strip()[8:]  # 删除开头的CVSS3.1

        return [cve_score, cve_level, cve_cvss]

    @staticmethod
    def __format_cve_level(level):
        cn_level = ['低危', '中危', '高危']
        en_level = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']
        out = 'ERROR_LEVEL'
        if level == en_level[0]:
            out = cn_level[0]
        elif level == en_level[1]:
            out = cn_level[1]
        elif level == en_level[2]:
            out = cn_level[2]
        elif level == en_level[3]:
            out = cn_level[2]
        return out
